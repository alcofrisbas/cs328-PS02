{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Architectures\n",
    "\n",
    " Cognitive architectures often use production rules for reasoning. Production rules are logical implications in the form *if* $\\alpha$, *then* $\\beta$; written in logic, $ \\alpha \\rightarrow \\beta$. If the system believes that $\\alpha$ is true, then it can conclude that $\\beta$ is true using this rule. \n",
    "    \n",
    "We'll implement a production rule system in Python. We first need a representation of the system's *rules* and its *beliefs*. To start, we'll keep things relatively simple and assume each rule consists of exactly one statement implying another statement, where the statements contain no logical connectives. We'll represent this as a two-item tuple, with the first item equal to the antecedent (if part) and the second item equal to the consequent (then part). For instance, the logical rule \"If gas meter at zero, then no gas in tank\", would be represented as `('gas meter at zero','no gas in tank')`. We'll represent our beliefs as a set of strings in Python. (If you haven't seen the syntax for sets in Python, [check out the documentation here](https://docs.python.org/3/tutorial/datastructures.html#sets). Most syntax is the same as a dictionary with only keys, no values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward chaining\n",
    "\n",
    "We'll start off using a *forward chaining* system, in which the system repeatedly uses the rules to determine if anything new can be added to its beliefs. For instance, if the system only had the belief \"gas meter at zero\" and the rule \"If gas meter at zero, then no gas in tank\", it could use forward chaining to add \"no gas in tank\" as an additional belief. The system is thus able to consider the logical consequences of its beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (a)</h3>\n",
    "<p>\n",
    "Create a module `productionrules.py`. Write a function `get_triggered_rule(beliefs, rules)` that finds a rule that is \"triggered\" by a given set of beliefs. This function should take two parameters: (1) the beliefs (a set of strings), and (2) a list of rules, where each rule is a two-item tuple. Your function should return the first rule that allows the system to add something new to its beliefs. If no such rule exists, the function should return None.\n",
    "      <p><p>  \n",
    "        For example, if the rule is \"If a, then b\" and the belief set contained a and b, then the rule is not triggered, because b is already part of the set of beliefs. If the belief set contained only b, then the rule is not triggered because the antecedent of the rule is not true. However, if the belief set contains only a, then the rule is triggered because this rule would allow us to conclude b, which we did not already know.\n",
    "        <p><p>\n",
    "        In the cell below, write some tests demonstrating that your `get_triggered_rule` function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (b)</h3>\n",
    "<p>\n",
    "In forward chaining, we keep adding to our beliefs by executing the triggered rules until no more rules are triggered. \"Executing a rule\" simply means adding its consequent to our beliefs. \n",
    "<p><p>\n",
    "Here's an example: We start with the rules \"If belief1, then belief2\" and \"If belief2, then belief3\", and the only belief in our belief set is \"belief1\". We first execute rule 1, increasing our belief set to contain \"belief1\" and \"belief2\". Then, with this expanded set of beliefs, rule 2 is trigger. We execute it so that our final set of beliefs is \"belief1\", \"belief2\", and \"belief3\".\n",
    "\n",
    "<p>\n",
    "In your module, write a function `forward_chain(beliefs, rules)`, to automatically perform forward chaining using a given set of initial beliefs and list of rules. This function should return a tuple where the first item is the final set of beliefs and the second item is a list of rules that were triggered, in the order they were triggered. This function should use the function you wrote in the previous part, and should not modify the passed in beliefs or rules (i.e., it should be non-destructive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward chaining example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Imagine you have an old car whose battery often dies without warning. When your car battery dies you can't charge your phone, turn on your headlights, or start the car. To make matters worse, you also happen to be quite forgetful and often neglect to fill up the gas tank when it is running low. When your gas tank is empty the gas meter on your dashboard reads zero and your car won't start. We can represent this scenario in a production system as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules1 = [(\"gas meter at zero\", \"no gas in tank\"),\n",
    "          (\"no gas in tank\", \"car won't start\"),\n",
    "          (\"car battery is dead\", \"head lights won't turn on\"),\n",
    "          (\"car battery is dead\",\n",
    "          \"phone charger doesn't work\"),\n",
    "          (\"car battery is dead\", \"car won't start\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (c)</h3>\n",
    "<p>\n",
    "In the cell above (with the rules), add code that uses your function(s) from above to run forward chaining given that you know that the car's gas meter is at zero (and you start off knowing nothing else). In the cell below, write down what you can conclude  about the states of the gas tank, head lights, phone charger, and car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward chaining example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's imagine a different production system that is similar but ultimately unrelated to the one we defined in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules2 = [(\"no gas in tank\", \"gas meter at zero\"),\n",
    "          (\"head lights won't turn on\", \"car battery is dead\"),\n",
    "          (\"my phone charger doesn't work\", \"car battery is dead\"),\n",
    "          (\"car won't start\", \"car battery is dead\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (d)</h3>\n",
    "<p>\n",
    "In this new production system, if you knew only that your car won't start what would you conclude?  Intuitively, do your conclusion(s) seem like valid or reasonable inferences to make given the rules of your production system? Explain in the cell below. Include code to support your conclusions if appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward chaining example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can imagine a third production system where the rules apply in both directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules3 = [(\"gas meter at zero\", \"no gas in tank\"),\n",
    "          (\"no gas in tank\", \"car won't start\"),\n",
    "          (\"car battery is dead\", \"head lights won't turn on\"),\n",
    "          (\"car battery is dead\", \"phone charger doesn't work\"),\n",
    "          (\"car battery is dead\", \"car won't start\"),\n",
    "          (\"no gas in tank\", \"gas meter at zero\"),\n",
    "          (\"head lights won't turn on\", \"car battery is dead\"),\n",
    "          (\"phone charger doesn't work\", \"car battery is dead\"),\n",
    "          (\"car won't start\", \"car battery is dead\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (e)</h3>\n",
    "<p>\n",
    "These rules are in \"Forward chain example 3\". If you observed \"no gas in tank\", what would you conclude? Does this conclusion seem reasonable given what you know about the meaning of the propositions? Briefly explain your reasoning. Include code to support your conclusions if appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits of production rule systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (f)</h3>\n",
    "<p>\n",
    "Do the parts above tell us anything about the limits of a basic production rule system for making inferences? In the cell below, explain how the system's conclusions are consistent with or different from the types of conclusions that people make. Your answer should include how you think people would behave given a situation similar to the one in earlier parts of the questions, and should discuss the types of conclusions people might draw given particular evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of reasoning used in production rule systems is backward chaining. In backward chaining, we still have beliefs and rules, but the basic method of inference is to begin with a goal proposition that we would like to prove, and then determine if we can prove that inference using our rules and beliefs. Some cognitive architectures, such as ICARUS (Langley & Choi, 2006) actually have the ability to do both types of reasoning.\n",
    "\n",
    "In backward chaining, rules and beliefs take the same form as for forward chaining. Additionally, the goal is simply another logical proposition; as above, we'll assume the statements contain no logical connectives, although the reasoning system can be applied in more complicated cases. The system first checks if its goal is one of its given beliefs. If it is, then the goal has been proven to be true. Otherwise, it finds any rules that have the goal as a consequent. For example, if we had the rules $[a \\rightarrow b, b \\rightarrow c, d \\rightarrow c]$, belief $a$, and goal $c$, the system would find the rules $[b \\rightarrow c, d \\rightarrow c]$. It then uses the antecedent (left side) of these rules as new goals. In this example, it's now trying to prove either $b$ or $d$. If repeats the process of checking if the (new) goal is one of its beliefs, and if not, finding rules that have the goal as a consequent. This process repeats until either there are no new rules to examine or a goal can be proven. In this example, the system would eventually have the goal of $a$ (since $a \\rightarrow b$ and $b\\rightarrow c$), and since this was a belief, it would recognize that it can prove the original goal $c$. \n",
    "<p>\n",
    "(Citation for ICARUS paper: Langley, P., & Choi, D. (2006) A unified cognitive architecture for physical agents. *In Proceedings of AAAI*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (a)</h3>\n",
    "<p>\n",
    "In `productionrules.py`, write a function `backward_chain(beliefs, rules, goal)` that returns true if the given goal (a string) can be proven with the given beliefs and rules. The same representation of beliefs and rules is used as in the previous problem. As in the previous problem, your function should be non-destructive. In the cell below write some test cases for your function. Make sure to test a variety of cases, including rule systems that may have cycles and cases where the goal cannot be proven as well as cases where the goal can be proven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chaining systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (b)</h3>\n",
    "<p>\n",
    "Consider whether one of backward or forward chaining is always more efficient if the system has a single proposition that it would like to evaluate as supported (proven true) or not by the current beliefs and rules. Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Uses for chaining systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3>Part (c)</h3>\n",
    "<p>\n",
    "Explain whether it seems like it would ever be useful for an agent to have both reasoning systems available. You can assume that the agent is acting in the environment for a prolonged period of time and it may gain information over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
